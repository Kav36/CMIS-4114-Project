{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvidia-smi -q -d POWER\n",
    "# trainer.train(resume_from_checkpoint=\"./results/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load SQuAD v2 dataset\n",
    "ds = load_dataset(\"rajpurkar/squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n",
      "{'id': '56be85543aeaaa14008c9063', 'title': 'Beyonc√©', 'context': 'Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyonc√©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n"
     ]
    }
   ],
   "source": [
    "# Check available splits\n",
    "print(ds)\n",
    "\n",
    "# Inspect a sample from the training set\n",
    "print(ds['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\kavin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74d7b1a72774000a6c228033dca1c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load and preprocess dataset as before\n",
    "ds = load_dataset(\"rajpurkar/squad_v2\")\n",
    "\n",
    "# Preprocessing function (as defined before)\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    \n",
    "    start_positions, end_positions = [], []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        if i >= len(answers) or \"answer_start\" not in answers[i] or len(answers[i][\"answer_start\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_char = answers[i][\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[i][\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            token_end_index = 0\n",
    "            for idx, (start, end) in enumerate(offsets):\n",
    "                if start <= start_char < end:\n",
    "                    token_start_index = idx\n",
    "                if start < end_char <= end:\n",
    "                    token_end_index = idx\n",
    "                    break\n",
    "            start_positions.append(token_start_index)\n",
    "            end_positions.append(token_end_index)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_ds = ds.map(preprocess_data, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "# Define data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizerFast, pipeline\n",
    "import os\n",
    "\n",
    "# Path to the directory containing checkpoints\n",
    "checkpoints_dir = \"./results\"\n",
    "\n",
    "# List all checkpoints and sort them by training step number\n",
    "checkpoints = [os.path.join(checkpoints_dir, d) for d in os.listdir(checkpoints_dir) if d.startswith(\"checkpoint\")]\n",
    "checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "\n",
    "# Example validation data\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country in Europe. The capital of France is Paris.\"\n",
    "\n",
    "best_checkpoint = None\n",
    "best_score = float(\"-inf\")\n",
    "\n",
    "print(\"Evaluating all checkpoints to find the best one...\")\n",
    "for checkpoint in checkpoints:\n",
    "    print(f\"Evaluating checkpoint: {checkpoint}\")\n",
    "    # Load model for this checkpoint\n",
    "    model = BertForQuestionAnswering.from_pretrained(checkpoint)\n",
    "    # Load tokenizer from original base model\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Create a pipeline for question answering\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Perform QA inference and evaluate the result\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Checkpoint: {checkpoint}, Answer: {result['answer']}, Score: {result['score']}\")\n",
    "\n",
    "    # Track the best checkpoint based on the score\n",
    "    if result['score'] > best_score:\n",
    "        best_score = result['score']\n",
    "        best_checkpoint = checkpoint\n",
    "\n",
    "# Output the best checkpoint\n",
    "print(f\"Best checkpoint: {best_checkpoint} with score {best_score}\")\n",
    "\n",
    "# Save the best checkpoint to a permanent location\n",
    "final_model_path = \"./final_model\"\n",
    "print(f\"Saving the best checkpoint to: {final_model_path}\")\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "model.save_pretrained(final_model_path)\n",
    "\n",
    "# Reload the saved model for verification\n",
    "print(\"Reloading the saved model for verification...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(final_model_path)\n",
    "model = BertForQuestionAnswering.from_pretrained(final_model_path)\n",
    "\n",
    "# Use the saved model for inference\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Final Answer: {result['answer']}, Score: {result['score']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
